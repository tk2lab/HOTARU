from logging import getLogger
from functools import partial

import jax
import jax.lax as lax
import jax.numpy as jnp
import numpy as np

from .dynamics import get_dynamics
from .optimizer import ProxOptimizer
from .penalty import get_penalty
from .prepare import prepare_matrix

logger = getLogger(__name__)


def spatial(data, x, dynamics, penalty, env, prepare, optimize, step):
    logger.info("spatial:")
    common = dynamics, penalty, env
    mat = prepare_matrix("spatial", data, *x, *common, **prepare)
    optimizer = get_optimizer("spatial", mat, *common, **optimize)
    optimizer.fit(**step)
    return np.array(jnp.concatenate(optimizer.x, axis=0))


def temporal(data, x, dynamics, penalty, env, prepare, optimize, step):
    logger.info("temporal:")
    common = dynamics, penalty, env
    mat = prepare_matrix("temporal", data, *x, *common, **prepare)
    optimizer = get_optimizer("temporal", mat, *common, **optimize)
    optimizer.fit(**step)
    return optimizer.val


@jax.jit
def loss_fwd(nn, nm, py, a, b, c, *x):
    nk, nx = a.shape
    x = jnp.row_stack(x)
    xdot = (a * x).sum()
    xsum = x.sum(axis=1)
    xmean = xsum / nx
    xcov = x @ x.T
    var = (nn + (xcov * b).sum() + (xsum @ c @ xsum) - 2 * xdot) / nm
    return jnp.log(var) / 2 + py, (x, var, xmean)


@jax.jit
def loss_bwd(nn, nm, py, a, b, c, r, g):
    x, var, xmean = r
    scale = g / nm / var
    cx = c @ xmean
    return tuple(scale * (b[i] @ x + cx[i] - a[i]) for i, xi in enumerate(x))


@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3, 4, 5))
def loss_fn(nn, nm, py, a, b, c, *x):
    return loss_fwd(nn, nm, py, a, b, c, *x)[0]


loss_fn.defvjp(loss_fwd, loss_bwd)


class Model:

    def __init__(self, kind, mat, dynamics, batch):
        nx, ny, n1, n2, py, a, b, c = mat
        nxf = jnp.array(nx, jnp.float32)
        nyf = jnp.array(ny, jnp.float32)
        nn = nxf * nyf
        nm = nn + nxf + nyf
        py = jnp.array(py, jnp.float32) / nm
        a = jnp.array(a)
        b = jnp.array(b)
        c = jnp.array(c)

        self.kind = kind
        self.shape = nx, ny, n1, n2
        self.dynamics = get_dynamics(dynamics)
        self.args = nn, nm, py, a, b, c
        self.scale = nm
        self.dmax = b.diagonal().max()

    def loss(self, *x):
        nx, ny, n1, n2 = self.shape
        if self.kind == "temporal":
            for i in range(n1):
                x[i] = self.dynamics(x[i])
        return loss_fn(*self.args, *x)


def get_optimizer(kind, mat, dynamics, penalty, env, factor, lr, scale):
    model = Model(kind, mat, dynamics, batch=10000)
    nx, ny, n1, n2 = model.shape
    penalty = get_penalty(penalty)
    match kind:
        case "spatial":
            init = [np.zeros((nx,))] * n1 + [np.zeros((nx,))] * n2
            pena = (penalty.la,) * n1 + (penalty.lx,) * n2
        case "temporal":
            nu = nx + dynamics.size - 1
            init = [np.zeros((nu,))] * n1 + [np.zeros((nx,))] * n2
            pena = (penalty.lu,) * n1 + (penalty.lt,) * n2
        case _:
            raise ValueError("kind must be temporal or spatial")

    opt = ProxOptimizer(model.loss, init, pena, f"{kind} optimize")
    opt.set_params(lr / model.dmax, scale, model.scale)
    return opt
